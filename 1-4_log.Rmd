---
title: "Simple Nonlinear Models"
output:
  html_document: 
    css: style.css
    theme: spacelab
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    toc_depth: 2
    code_download: no
    highlight: tango
link-citations: true
---

```{r child = '_head.Rmd'}
```

# Introduction {.fixed-box}

In this tutorial, we will evaluate basic nonlinear relationships model. 

### Load data and estimate model

For this tutorial, we will be using the `food` data from PoE that can be found at http://www.principlesofeconometrics.com/poe5/poe5data.html. To make it easier to access this data, I have set up a shortened URL: http://tiny.cc/poe
-data. Thus, to load `food.csv` into R, use the following URL: http://tiny.cc/poe-data/food.csv. We use the following code to load the data and estimate the model we will use in this tutorial.

```{r, echo=T, results='hide'}
food <- read.csv(url("http://tiny.cc/poe-data/food.csv"))
mod1 <- lm(food_exp ~ income, data = food)
```

# Functional Forms {.tabset .fixed-height}

## Lecture


## Explanation

### Some basic functional forms
```{r echo=F}
x <- seq(from=0,to=10,length.out = 100)
c <- 10

quad <- data.frame(x=x) %>% 
  ggplot(aes(x)) +
  geom_function(fun=function(x) 10 - (x-5)^2, color="red") +
  geom_function(fun=function(x) 5 + (x-5)^2, color="blue") +
  theme_classic() +
  ggtitle("Quadratic") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())

cube <- data.frame(x=x) %>% 
  ggplot(aes(x)) +
  geom_function(fun=function(x) (1/3)*(0.4*x-2)^3 +5, color="blue") +
  theme_classic() +
  ggtitle("Cubic") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())

log.small <- data.frame(x=x) %>% 
  ggplot(aes(x)) +
  geom_function(fun=function(x) exp(c)*x^-.5, color="red") +
  geom_function(fun=function(x) exp(c)*x^+.5, color="blue") +
  theme_classic() +
  ggtitle("Logarithmic") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())

log.exp <- data.frame(x=x) %>% 
  ggplot(aes(x)) +
  geom_function(fun=function(x) exp(c)*x^-1.5, color="red") +
  geom_function(fun=function(x) exp(c)*x^+1.5, color="blue") +
  theme_classic() +
  ggtitle("Exponential") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank())

multiplot(quad,cube,log.small,log.exp, cols=2)

```

Above is a small sample of the functional forms that can be used to fit data. To determine the proper model, you need to consider the following:

1. the theoretical relationship between the variables
2. the plots of the data
3. the residual diagnostics

### Common functional forms used in regression

In the table below, I have listed some common transformations applied to data and how thoes transformation effect the interpretaition of the regression coefficents.

::: {#Table1}
|Name|Function|Slope = $\frac{{\delta y}}{{\delta x}}$|Elasticity = $\frac{{\% \Delta y}}{{\% \Delta x}}$|
|:---------|:-----------------|:----------|:-----------|
|Linear | $y = {\beta_1} + {\beta_2}x + \varepsilon$ | $\beta _2$| ${\beta _2}\frac{x}{y}$ |
|Quadratic | $y = {\beta _1} + {\beta _2}{x^2} + \varepsilon$ | $2{\beta _2}x$|$2{\beta _2}x\frac{x}{y}$ |
|Cubic| $y = {\beta _1} + {\beta _2}{x^3} + \varepsilon$ |  $3{\beta _2}x^2$|$3{\beta _2}x^2\frac{x}{y}$ |
|Log-Log | $\log (y) = {\beta _1} + {\beta _2}\log (x) + \varepsilon$ | ${\beta _2}\frac{y}{x}$ | ${\beta _2}$|
|Log-Linear | $\log (y) = {\beta _1} + {\beta _2}x + \varepsilon$ | ${\beta _2}y$ | ${\beta _2}x$|
|Linear-Log | $y = {\beta _1} + {\beta _2}\log (x) + \varepsilon$ | ${\beta _2}\frac{1}{x}$ | ${\beta _2}\frac{1}{y}$|

Table: Coefficient Interpretation
:::

### Let's try some example regressions

**Linear-Log Models**

In linear-log models, the dependent variable $y$ is regressed on the natural log of the independent variables, i.e. $y = {\beta _1} + {\beta _2}\log (x) + \varepsilon$.

```{r}
mod2 <- lm(food_exp~log(income), data=food)
summary(mod2)
```

We can use `ggplot()` to draw a scater plot of the data with the regression curve superimposed. 
```{r}
food %>% ggplot(aes(x=income, y=food_exp)) +
  geom_point() +
  geom_smooth(method=lm, formula=y~log(x), se=TRUE) +
  theme_classic() 
```
And we can use the `plot.fitted()` function we defined previously to draw the Residual vs. Fit Plot.

```{r}
plot.fitted(mod2)
```

::: {.info-box .tip}
The the two plots above indicate that the linear log model is better than the a linear model, but the linear log model is certainly not perfect.
:::

**More models with logs**

```{r}
mod3 <- lm(log(food_exp)~income, data=food)
summary(mod3)
food %>% ggplot(aes(x=income, y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()
```



```{r}
mod4 <- lm(log(food_exp)~log(income), data=food)
summary(mod4)
food %>% ggplot(aes(x=log(income), y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()
```


## You try

Estimate a log-linear model and store it in an object named `mod3`, plot income vs. log of food expenditure with the regression line superimposed, and draw the Residuals vs. Fitted Plot.


```{r tut=T, ex="logLinEX", type="pre-exercise-code"}
food <- read.csv(url("http://tiny.cc/poe-data/food.csv"))
library(ggplot2)
library(dplyr)
plot.fitted <- function(lmod) {
  df = lmod$model
  title = paste("Residuals vs. Fits (Dependent Variable: ",
                names(df)[1],
                ")",
                sep = "")
  pr <- df %>% ggplot(aes(x = fitted(lmod), y = resid(lmod))) +
    geom_ribbon(aes(ymin = -sigma(lmod), ymax = sigma(lmod)),
                fill = "gray",
                alpha = .5) +
    geom_ribbon(aes(ymin = -sigma(lmod) * 2, ymax = sigma(lmod) * 2),
                fill = "lightgray",
                alpha = .5) +
    geom_point() +
    theme_classic() +
    ggtitle(title) +
    ylab("Residules") +
    xlab("Fitted") +
    geom_hline(yintercept = 0,
               linetype = "solid",
               color = "black")
  return(pr)  
}  
```

```{r tut=T, ex="logLinEX", type="sample-code"}
# Estimate a log-linear model and store it in an object named `mod3`



# Plot income vs. log of food expenditure with the regression line superimposed



# Draw the Residuals vs. Fitted Plot



```

```{r tut=T, ex="logLinEX", type="solution"}
# Estimate a log-linear model and store it in an object named `mod3`
mod3 <- lm(log(food_exp)~income, data=food)
summary(mod3)
# Plot income vs. log of food expenditure with the regression line superimposed
food %>% ggplot(aes(x=income, y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()
# Draw the Residuals vs. Fitted Plot
plot.fitted(mod3)
```


```{r tut=T, ex="logLinEX", type="sct"}
test_object("mod3")
success_msg(praise("${Exclamation}! You did a ${adjective} job!"))
```


# Residuals vs. Fitted Plot {.fixed-height}



```{r tut=T, ex="logEX", type="pre-exercise-code"}
food <- read.csv(url("http://tiny.cc/poe-data/food.csv"))
library(ggplot2)
library(dplyr)
plot.fitted <- function(lmod) {
  df = lmod$model
  title = paste("Residuals vs. Fits (Dependent Variable: ",
                names(df)[1],
                ")",
                sep = "")
  pr <- df %>% ggplot(aes(x = fitted(lmod), y = resid(lmod))) +
    geom_ribbon(aes(ymin = -sigma(lmod), ymax = sigma(lmod)),
                fill = "gray",
                alpha = .5) +
    geom_ribbon(aes(ymin = -sigma(lmod) * 2, ymax = sigma(lmod) * 2),
                fill = "lightgray",
                alpha = .5) +
    geom_point() +
    theme_classic() +
    ggtitle(title) +
    ylab("Residules") +
    xlab("Fitted") +
    geom_hline(yintercept = 0,
               linetype = "solid",
               color = "black")
  return(pr)  
}  
```

```{r tut=T, ex="logEX", type="sample-code"}
# Estimate a log-linear model and store it in an object named `mod3`



# Plot income vs. log of food expenditure with the regression line superimposed



# Draw the Residuals vs. Fitted Plot



```

```{r tut=T, ex="logEX", type="solution"}
# Estimate a log-linear model and store it in an object named `mod3`
mod3 <- lm(log(food_exp)~income, data=food)
summary(mod3)
# Plot income vs. log of food expenditure with the regression line superimposed
food %>% ggplot(aes(x=income, y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()
# Draw the Residuals vs. Fitted Plot
plot.fitted(mod3)
```


```{r tut=T, ex="logEX", type="sct"}
test_object("mod3")
success_msg(praise("${Exclamation}! You did a ${adjective} job!"))
```


# Prediction {.tabset .fixed-height}

## Lecture


## Explanition

The log-linear model, $\log(y) = \beta_1 + \beta_2 x$, has a logarithmic term on the left-hand side of the
equation; thus, to predict $y$ a natural choice for prediction is 

$$\hat y = \exp \left[ {\log (y)} \right] = \exp \left( {{b_1} + {b_2}x} \right)$$
But this prediction is known to be biased. To correct for the bias, we can multiply $\hat{y}$ the following correction factor $e^{\hat{\sigma}^2/2}$, so that 
$$\hat{y}_c=\hat{y}e^{\hat{\sigma}^2/2}=\exp \left( {{b_1} + {b_2}x} + \hat{\sigma}^2/2 \right). $$
The following code calculates both the corrected and uncorrected prediction of food expenditure from the log-linear model.

```{r}
mod3 <- lm(log(food_exp)~income, data=food)
b1 <- coef(mod3)[[1]]
b2 <- coef(mod3)[[2]]

smod3<-summary(mod3)
sighat2 <- smod3$sigma^2

yhat <- exp(b1+b2*food$income)
yhatc <- exp(b1+b2*food$income + sighat2/2)
```

And we can quickly plot the model as follows.

```{r}
food %>% ggplot() +
  geom_point(aes(x=income, y=food_exp)) +
  geom_line(aes(x=income,y=yhat, linetype="Yhat", color="Yhat")) +
  geom_line(aes(x=income,y=yhatc, linetype="Yhatc", color="Yhatc")) +
  theme_classic()
```

## You try

Instructions

```{r tut=T, ex="predEX", type="pre-exercise-code"}
#food <- read.csv(url("http://tiny.cc/poe-data/food.csv"))
#mod1 <- lm(food_exp ~ income, data = food)
```

```{r tut=T, ex="predEX", type="sample-code"}
# 

```

```{r tut=T, ex="predEX", type="solution"}
#plot(x=food$food_exp,y=mod1$fitted.values)
#abline(0,1)
```


```{r tut=T, ex="predEX", type="sct"}
#test_object("mod1.prod")
#success_msg(praise("${Exclamation}! You did a ${adjective} job!"))
```


# Summary of all code {.fixed-box}

```{r echo=T, eval=F}
mod2 <- lm(food_exp~log(income), data=food)
summary(mod2)

food %>% ggplot(aes(x=income, y=food_exp)) +
  geom_point() +
  geom_smooth(method=lm, formula=y~log(x), se=TRUE) +
  theme_classic() 

plot.fitted(mod2)

mod3 <- lm(log(food_exp)~income, data=food)
summary(mod3)
food %>% ggplot(aes(x=income, y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()

mod4 <- lm(log(food_exp)~log(income), data=food)
summary(mod4)
food %>% ggplot(aes(x=log(income), y=log(food_exp))) +
  geom_point() +
  geom_smooth(method=lm, se=TRUE) +
  theme_classic()
```



# You did it!{.fixed-box}

In the next tutorial, we will learn about accessing Goodness-of-Fit. The next lesson is available here: https://logan-kelly.shinyapps.io/4-2_GoF/

![](img/great-job-star-drawing.jpeg){width="75%"}